{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b409c7fc-b548-4d7a-8905-bbd5b26b999d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_path=Path('/Users/maksimtrebusinin/Desktop/Cancer_data')\n",
    "list_dir=os.listdir(data_path)[1:]\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3172f5-7a09-40ca-b461-1af6642b59ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067b3878-a987-4d17-8e89-3830cd6c9f1d",
   "metadata": {},
   "source": [
    "X=list()\n",
    "y=[0]*4000\n",
    "index=0\n",
    "for i, column_name in enumerate(list_dir):\n",
    "    for img_path in glob.iglob(os.path.join(data_path, column_name, '*')):\n",
    "        X.append(np.asarray(mpimg.imread(img_path)))\n",
    "        y[index]=i\n",
    "        index+=1\n",
    "#0 is Healthy\n",
    "#1 is Melanoma\n",
    "#2 is Bascal cell carcioma\n",
    "#3 is Squamous cell carcinoma\n",
    "y=np.array(y)\n",
    "transfrom_X=[transform.resize(image, (159,159,3)) for image in X]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5e8e8a-f8ff-4327-8aa6-fde482713f6b",
   "metadata": {},
   "source": [
    "# train and test selections\n",
    "X_train, X_test, y_train, y_test = train_test_split(transfrom_X, y, test_size=0.25, stratify=y,\n",
    "                                                    random_state=42)\n",
    "X_train_tensor=torch.FloatTensor(X_train)\n",
    "X_test_tensor=torch.FloatTensor(X_test)\n",
    "y_train_tensor=torch.from_numpy(y_train)\n",
    "y_test_tensor=torch.from_numpy(y_test)\n",
    "\n",
    "X_train_tensor=X_train_tensor.reshape(3000, 3, 159, 159)\n",
    "X_test_tensor=X_test_tensor.reshape(1000, 3, 159, 159)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e18f9a4f-2b9e-4a34-90d7-8f093a7b51ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "430a782b-1606-461c-bd94-cbdf57b777bd",
   "metadata": {},
   "source": [
    "class CancerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerNet, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=30, kernel_size=4)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, padding=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=60, out_channels=120, kernel_size=2)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act3 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=120, out_channels=80, kernel_size=3, padding=1)\n",
    "        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act4 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(in_channels=80, out_channels=30, kernel_size=3, padding=1)\n",
    "        self.act5 = torch.nn.ReLU()\n",
    "\n",
    "        # Добавляем Dropout после последней активации перед полносвязным слоем\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.5)  # 50% случайных обнулений\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(9*9*30, 200)\n",
    "        self.act6 = torch.nn.ReLU()\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.5)  # 50% Dropout на скрытом слое\n",
    "        self.fc2 = torch.nn.Linear(200, 4)\n",
    "        self.sm = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.act5(x)\n",
    "\n",
    "        # Применяем Dropout после всех сверток и активаций\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))  # Преобразуем в вектор для подачи на fc\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act6(x)\n",
    "        \n",
    "        # Применяем Dropout после первого полносвязного слоя\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def inference(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "627c58d2-6826-43d7-8fb5-e027b8bbcc3b",
   "metadata": {},
   "source": [
    "cancer_net=CancerNet()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(cancer_net.parameters(), lr=0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b46cbc64-1fbb-4db2-9486-0f587dfd9a41",
   "metadata": {},
   "source": [
    "def train_model(model, X_tr, y_tr, epoch, batch_size, opt, loss_fn):\n",
    "    start_time = time.time()  # Начинаем отсчёт времени\n",
    "    \n",
    "    # Преобразуем данные в TensorDataset и используем DataLoader\n",
    "    dataset = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Инициализация переменных для отслеживания потерь\n",
    "    for ep in range(epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0.0  # Сумма потерь за одну эпоху\n",
    "        model.train()  # Устанавливаем модель в режим тренировки\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Прогоняем батч через модель\n",
    "            pred = model(X_batch)\n",
    "            \n",
    "            # Вычисляем потерю\n",
    "            loss_value = loss_fn(pred, y_batch)\n",
    "            total_loss += loss_value.item()\n",
    "\n",
    "            # Обратное распространение и шаг оптимизации\n",
    "            loss_value.backward()\n",
    "            opt.step()\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        hours, rem = divmod(epoch_duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        # Выводим статистику о потере и времени\n",
    "        print(f\"Epoch {ep + 1}/{epoch}, Loss: {total_loss / len(train_loader):.4f}, Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "    \n",
    "    total_duration = time.time() - start_time  # Общее время тренировки\n",
    "    hours, rem = divmod(total_duration, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    print(f\"Training completed in: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "\n",
    "def predict(model, X_test):\n",
    "    model.eval()  # Переводим модель в режим инференса (выключается Dropout и BatchNorm)\n",
    "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "        predictions = model.forward(X_test_tensor) # Получаем прогнозы\n",
    "        predictions = predictions.argmax(dim=1)\n",
    "        predictions = np.array(predictions)\n",
    "        return predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5803688-b67c-44c2-a9a1-aa31c8f51b93",
   "metadata": {},
   "source": [
    "train_model(cancer_net, X_train_tensor,y_train_tensor, 100, 64, optimizer, loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05b24bbb-a23e-42f0-9232-9ce4b44fb58d",
   "metadata": {},
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    print('Accuracy score:', accuracy_score(y_true, y_pred))\n",
    "    mat=confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(mat, annot=True, fmt='d', cmap='coolwarm')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40edcf61-69fd-4dd6-bfc1-d43969953885",
   "metadata": {},
   "source": [
    "y_pred = predict(cancer_net, X_test_tensor)\n",
    "print(y_pred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f0e4399-8210-4f7a-b7e9-5c79b2f59d6c",
   "metadata": {},
   "source": [
    "metrics(y_test, np.array(y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52bc42cf-6e0a-4c47-be4a-bb7100cd8046",
   "metadata": {},
   "source": [
    "cancer_netV2=CancerNet()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(cancer_netV2.parameters(), lr=0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbb772f3-4299-46d3-8c1a-45c33e492bf6",
   "metadata": {},
   "source": [
    "train_model(cancer_netV2, X_train_tensor,y_train_tensor, 90, 256, optimizer, loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5481c545-b2ac-4282-8c1e-ec1c5a8332f0",
   "metadata": {},
   "source": [
    "y_pred2 = predict(cancer_netV2, X_test_tensor)\n",
    "metrics(y_test, np.array(y_pred2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9dce999-320a-484b-8b69-5eb41b0a615f",
   "metadata": {},
   "source": [
    "cancer_netV3=CancerNet()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(cancer_netV3.parameters(), lr=0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b73730b-3b19-4b6f-868e-3303de0a070a",
   "metadata": {},
   "source": [
    "train_model(cancer_netV3, X_train_tensor,y_train_tensor, 100, 32, optimizer, loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee8c0ac2-7a0e-4670-89cc-72935a77edfe",
   "metadata": {},
   "source": [
    "y_pred3 = predict(cancer_netV3, X_test_tensor)\n",
    "metrics(y_test, np.array(y_pred3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cc1aea1-bee5-49e5-ab7a-6737c65a7b06",
   "metadata": {},
   "source": [
    "class CancerNetV2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerNetV2, self).__init__()\n",
    "\n",
    "        # Свёрточные слои с BatchNorm\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=30, kernel_size=4)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(30)  # Пакетная нормализация\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(60)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=60, out_channels=120, kernel_size=2)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(120)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act3 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=120, out_channels=80, kernel_size=3, padding=1)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(80)\n",
    "        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.act4 = torch.nn.ReLU()\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(in_channels=80, out_channels=30, kernel_size=3, padding=1)\n",
    "        self.bn5 = torch.nn.BatchNorm2d(30)\n",
    "        self.act5 = torch.nn.ReLU()\n",
    "\n",
    "        # Dropout после сверток\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        # Полносвязные слои с BatchNorm\n",
    "        self.fc1 = torch.nn.Linear(9*9*30, 200)\n",
    "        self.bn_fc1 = torch.nn.BatchNorm1d(200)\n",
    "        self.act6 = torch.nn.ReLU()\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.5)  # Dropout на скрытом слое\n",
    "        self.fc2 = torch.nn.Linear(200, 4)\n",
    "        self.sm = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Применяем BatchNorm\n",
    "        x = self.pool1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.act5(x)\n",
    "\n",
    "        # Применяем Dropout\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Преобразуем тензор для подачи на полносвязный слой\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)  # BatchNorm для полносвязного слоя\n",
    "        x = self.act6(x)\n",
    "\n",
    "        # Dropout на скрытом слое\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def inference(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "    def l1_regularization(self, model, lambda_l1=0.001):\n",
    "        # Рассчитываем L1 регуляризацию\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        return lambda_l1 * l1_norm\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42309614-816a-404d-9d3e-ef449e80e0fb",
   "metadata": {},
   "source": [
    "def train_model_for_L1(model, X_tr, y_tr, epoch, batch_size, opt, loss_fn, lambda_l1=0.004):\n",
    "    start_time = time.time()  # Начинаем отсчёт времени\n",
    "    \n",
    "    # Преобразуем данные в TensorDataset и используем DataLoader\n",
    "    dataset = TensorDataset(X_tr, y_tr)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Инициализация переменных для отслеживания потерь\n",
    "    for ep in range(epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0.0  # Сумма потерь за одну эпоху\n",
    "        model.train()  # Устанавливаем модель в режим тренировки\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Прогоняем батч через модель\n",
    "            pred = model(X_batch)\n",
    "            \n",
    "            # Вычисляем основную потерю\n",
    "            loss_value = loss_fn(pred, y_batch)\n",
    "\n",
    "            # Вычисляем L1 регуляризацию\n",
    "            l1_loss = model.l1_regularization(model, lambda_l1)\n",
    "\n",
    "            # Общая потеря = основная потеря + L1 регуляризация\n",
    "            total_loss_value = loss_value + l1_loss\n",
    "            total_loss += total_loss_value.item()\n",
    "\n",
    "            # Обратное распространение и шаг оптимизации\n",
    "            total_loss_value.backward()\n",
    "            opt.step()\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        hours, rem = divmod(epoch_duration, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "        # Выводим статистику о потере и времени\n",
    "        print(f\"Epoch {ep + 1}/{epoch}, Loss: {total_loss / len(train_loader):.4f}, Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "    \n",
    "    total_duration = time.time() - start_time  # Общее время тренировки\n",
    "    hours, rem = divmod(total_duration, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    print(f\"Training completed in: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c68c1391-10fa-4252-b239-aa426998fbc1",
   "metadata": {},
   "source": [
    "cancer_netV4=CancerNetV2()\n",
    "loss=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(cancer_netV4.parameters(), lr=0.001)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1697b59d-03ed-4c6f-8079-2f42c82589a6",
   "metadata": {},
   "source": [
    "train_model_for_L1(cancer_netV4, X_train_tensor, y_train_tensor, 200, 256, optimizer, loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bb0e6d1b-3c78-41d0-bc95-e5f0e006c9d8",
   "metadata": {},
   "source": [
    "y_pred4 = predict(cancer_netV4, X_test_tensor)\n",
    "metrics(y_test, np.array(y_pred4))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7916585f-e817-46c3-8f6d-136a40764e5e",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "45b44841-eb61-4bbd-8d61-1532907c8b6d",
   "metadata": {},
   "source": [
    "y_pred2 = predict(cancer_netV2, X_test_tensor)\n",
    "metrics(y_test, np.array(y_pred2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2c0b76ed-c74e-406b-9995-ade39d934c05",
   "metadata": {},
   "source": [
    "torch.save(cancer_netV2.state_dict(), 'CancerNet7.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef3378-14aa-41e5-8632-0274dd3b9747",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
